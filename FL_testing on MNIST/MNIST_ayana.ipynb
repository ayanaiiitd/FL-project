{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split train and test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset Train set 01 and Train set 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset organized as requested.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths for the output folders\n",
    "output_root = \"Ayana_Bharti\"\n",
    "mnist_01_path = os.path.join(output_root, \"train_mnist_01\")\n",
    "mnist_23_path = os.path.join(output_root, \"train_mnist_23\")\n",
    "\n",
    "# Create the output folders\n",
    "os.makedirs(mnist_01_path, exist_ok=True)\n",
    "os.makedirs(mnist_23_path, exist_ok=True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root=\"./\", download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Organize the dataset into the specified folders\n",
    "for image, label in mnist_dataset:\n",
    "    if label in [0, 1]:\n",
    "        folder = os.path.join(mnist_01_path, f\"train_image{label}_folder\")\n",
    "    elif label in [2, 3]:\n",
    "        folder = os.path.join(mnist_23_path, f\"train_image{label}_folder\")\n",
    "    else:\n",
    "        continue  # Skip other labels\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image = transforms.ToPILImage()(image)  # Convert the image to PIL format\n",
    "    image.save(os.path.join(folder, f\"{label}_{len(os.listdir(folder))}.png\"), \"PNG\")\n",
    "\n",
    "print(\"MNIST dataset organized as requested.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set 012 and 234"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST dataset 012 and 234\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the paths for the output folders\n",
    "output_root = \"Ayana_Bharti\"\n",
    "mnist_012_path = os.path.join(output_root, \"train_mnist_012\")\n",
    "mnist_234_path = os.path.join(output_root, \"train_mnist_234\")\n",
    "\n",
    "# Create the output folders\n",
    "os.makedirs(mnist_012_path, exist_ok=True)\n",
    "os.makedirs(mnist_234_path, exist_ok=True)\n",
    "\n",
    "# Load the MNIST dataset\n",
    "mnist_dataset = datasets.MNIST(root=\"./\", download=True, train=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Organize the dataset into the specified folders\n",
    "for image, label in mnist_dataset:\n",
    "    if label in [0, 1, 2]:\n",
    "        folder = os.path.join(mnist_012_path, f\"train_image{label}_folder\")\n",
    "    if label in [2, 3, 4]:\n",
    "        folder = os.path.join(mnist_234_path, f\"train_image{label}_folder\")\n",
    "    else:\n",
    "        continue  # Skip other labels\n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image = transforms.ToPILImage()(image)  # Convert the image to PIL format\n",
    "    image.save(os.path.join(folder, f\"{label}_{len(os.listdir(folder))}.png\"), \"PNG\")\n",
    "\n",
    "print(\"MNIST dataset 012 and 234\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data 01 and 23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST test dataset 01 and 23.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "output_root = \"Ayana_Bharti\"\n",
    "mnist_01_path = os.path.join(output_root, \"test_mnist_01\")\n",
    "mnist_23_path = os.path.join(output_root, \"test_mnist_23\")\n",
    "\n",
    "os.makedirs(mnist_01_path, exist_ok=True)\n",
    "os.makedirs(mnist_23_path, exist_ok=True)\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root=\"./\", download=True, train=False, transform=transforms.ToTensor())\n",
    "for image, label in mnist_dataset:\n",
    "    if label in [0, 1]:\n",
    "        folder = os.path.join(mnist_01_path, f\"test_image{label}_folder\")\n",
    "    elif label in [2, 3]:\n",
    "        folder = os.path.join(mnist_23_path, f\"test_image{label}_folder\")\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image = transforms.ToPILImage()(image)  # Convert the image to PIL format\n",
    "    image.save(os.path.join(folder, f\"{label}_{len(os.listdir(folder))}.png\"), \"PNG\")\n",
    "print(\"MNIST test dataset 01 and 23.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data 134 and 345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST test dataset 012 and 234.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "output_root = \"Ayana_Bharti\"\n",
    "mnist_134_path = os.path.join(output_root, \"test_mnist_134\")\n",
    "mnist_345_path = os.path.join(output_root, \"test_mnist_345\")\n",
    "\n",
    "os.makedirs(mnist_134_path, exist_ok=True)\n",
    "os.makedirs(mnist_345_path, exist_ok=True)\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root=\"./\", download=True, train=False, transform=transforms.ToTensor())\n",
    "for image, label in mnist_dataset:\n",
    "    if label in [1, 3, 4]:\n",
    "        folder = os.path.join(mnist_134_path, f\"test_image{label}_folder\")\n",
    "    if label in [3, 4, 5]:\n",
    "        folder = os.path.join(mnist_345_path, f\"test_image{label}_folder\")\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image = transforms.ToPILImage()(image)  # Convert the image to PIL format\n",
    "    image.save(os.path.join(folder, f\"{label}_{len(os.listdir(folder))}.png\"), \"PNG\")\n",
    "print(\"MNIST test dataset 134 and 345.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST test dataset 012 and 234.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "output_root = \"Ayana_Bharti\"\n",
    "mnist_134_path = os.path.join(output_root, \"test_mnist_012\")\n",
    "\n",
    "os.makedirs(mnist_134_path, exist_ok=True)\n",
    "os.makedirs(mnist_345_path, exist_ok=True)\n",
    "\n",
    "mnist_dataset = datasets.MNIST(root=\"./\", download=True, train=False, transform=transforms.ToTensor())\n",
    "for image, label in mnist_dataset:\n",
    "    if label in [0, 1, 2]:\n",
    "        folder = os.path.join(mnist_134_path, f\"test_image{label}_folder\")\n",
    "    else:\n",
    "        continue \n",
    "\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    image = transforms.ToPILImage()(image)  # Convert the image to PIL format\n",
    "    image.save(os.path.join(folder, f\"{label}_{len(os.listdir(folder))}.png\"), \"PNG\")\n",
    "print(\"MNIST test dataset 012 and 234.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Maps 8 features to 2 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(14 * 14, 4)\n",
    "        self.fc2 = nn.Linear(4, 8)\n",
    "        self.fc3 = nn.Linear(8, 2)  # Two output classes (0/1 or 2/3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "def train_model(model, dataloader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        for inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        accuracy = correct / total_samples\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss}, Accuracy: {accuracy * 100}%\")\n",
    "    print(\"Training complete.\")\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define your data transforms (you can adjust these as needed)\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((14, 14)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Define the data loaders for the two folders\n",
    "dataset_01 = ImageFolder('Ayana_Bharti/train_mnist_01', transform=data_transform)\n",
    "dataset_23 = ImageFolder('Ayana_Bharti/train_mnist_23', transform=data_transform)\n",
    "\n",
    "# Define data loaders\n",
    "batch_size = 64\n",
    "loader_01 = DataLoader(dataset_01, batch_size=batch_size, shuffle=True)\n",
    "loader_23 = DataLoader(dataset_23, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test loader for client 1\n",
    "test_dataset_01 = ImageFolder('Ayana_Bharti/test_mnist_01', transform=data_transform)\n",
    "batch_size = 64\n",
    "test_loader_01 = DataLoader(test_dataset_01, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset_23 = ImageFolder('Ayana_Bharti/test_mnist_23', transform=data_transform)\n",
    "batch_size = 64\n",
    "test_loader_23 = DataLoader(test_dataset_23, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_01 = MLP()\n",
    "# train_model(model_01, loader_01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_23 = MLP()\n",
    "# train_model(model_23, loader_23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Federated Averaging (FedAvg) step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def server_averaging(model_01, model_23):\n",
    "    server_model = MLP()\n",
    "\n",
    "    # Extract parameters from the trained local models\n",
    "    model_01_params = model_01.state_dict()\n",
    "    model_23_params = model_23.state_dict()\n",
    "\n",
    "    # Perform parameter averaging\n",
    "    averaged_params = {}\n",
    "    for key in model_01_params:\n",
    "        averaged_params[key] = (model_01_params[key] + model_23_params[key]) / 2.0\n",
    "\n",
    "    # Load the averaged parameters into the server model\n",
    "    server_model.load_state_dict(averaged_params)\n",
    "\n",
    "    # Save the server model parameters to a file\n",
    "    server_model_param = server_model.state_dict()\n",
    "    os.makedirs('textfiles', exist_ok=True)  # Ensure the directory exists\n",
    "    torch.save(server_model_param, 'textfiles/server.txt')\n",
    "\n",
    "    return server_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ImageFolder('Ayana_Bharti/test_mnist_01', transform=data_transform)\n",
    "batch_size = 64\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IID Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def IID_simulation(local_epochs, Global_round, lr, train_loader1 , train_loader2, testloader):\n",
    "#     model_01 = MLP()\n",
    "#     model_23 = MLP()\n",
    "#     if os.path.isfile('server.txt'):\n",
    "#         model_01.load_state_dict(torch.load('server.txt'))\n",
    "#         model_23.load_state_dict(torch.load('server.txt'))\n",
    "#     for i in range(Global_round):\n",
    "#         w_1 = train_model(model_01, train_loader1, local_epochs, lr)\n",
    "#         w_2 = train_model(model_23, train_loader2, local_epochs, lr)\n",
    "#         w_avg = server_averaging(w_1, w_2)\n",
    "#         acc = test_accuracy(w_avg, testloader)\n",
    "#         print('Global Round: ', i+1, 'Acc: ', acc)\n",
    "#         with open ('textfiles/accuracy.txt','a') as f:\n",
    "#             f.write(str(acc)+'\\n')\n",
    "#     return acc\n",
    "\n",
    "\n",
    "def IID_simulation(local_epochs, Global_round, lr, train_loader1, train_loader2, testloader):\n",
    "    # Initialize the global model\n",
    "    initial_model = MLP()\n",
    "    if os.path.isfile('textfiles/server.txt'):\n",
    "        # Load the saved server model weights if they exist\n",
    "        initial_model.load_state_dict(torch.load('textfiles/server.txt'))\n",
    "\n",
    "    # Initialize local models\n",
    "    model_01 = MLP()\n",
    "    model_23 = MLP()\n",
    "\n",
    "    for i in range(Global_round):\n",
    "        # Reinitialize local models with the current global model weights\n",
    "        model_01.load_state_dict(initial_model.state_dict())\n",
    "        model_23.load_state_dict(initial_model.state_dict())\n",
    "\n",
    "        # Train the local models\n",
    "        train_model(model_01, train_loader1, local_epochs, lr)\n",
    "        train_model(model_23, train_loader2, local_epochs, lr)\n",
    "\n",
    "        # Perform server averaging\n",
    "        initial_model = server_averaging(model_01, model_23)\n",
    "\n",
    "        # Test the global model\n",
    "        acc = test_accuracy(initial_model, testloader)\n",
    "        print(f'Global Round: {i + 1}, Acc: {acc}')\n",
    "\n",
    "        # Save the accuracy to a file for tracking\n",
    "        os.makedirs('textfiles', exist_ok=True)\n",
    "        with open('textfiles/accuracy.txt', 'a') as f:\n",
    "            f.write(f'Round {i + 1}: {acc}\\n')\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ayana\\AppData\\Local\\Temp\\ipykernel_16872\\2797425286.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  initial_model.load_state_dict(torch.load('textfiles/server.txt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 299.5777939260006, Accuracy: 93.94657191735755%\n",
      "Epoch 2/2, Loss: 283.0023579597473, Accuracy: 94.4729569680221%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 367.9023208618164, Accuracy: 69.19513607411697%\n",
      "Epoch 2/2, Loss: 356.6672412753105, Accuracy: 72.31091625996085%\n",
      "Training complete.\n",
      "Global Round: 1, Acc: 94.70449172576832\n",
      "Epoch 1/2, Loss: 290.18180507421494, Accuracy: 94.19923674167654%\n",
      "Epoch 2/2, Loss: 273.6693693995476, Accuracy: 94.75983682063429%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 363.86761021614075, Accuracy: 70.11608349187966%\n",
      "Epoch 2/2, Loss: 352.1732311844826, Accuracy: 73.19601841894836%\n",
      "Training complete.\n",
      "Global Round: 2, Acc: 95.177304964539\n",
      "Epoch 1/2, Loss: 280.69332164525986, Accuracy: 94.46506119226214%\n",
      "Epoch 2/2, Loss: 264.43630304932594, Accuracy: 94.9467035136202%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 359.3647949695587, Accuracy: 71.20798522072408%\n",
      "Epoch 2/2, Loss: 347.39319866895676, Accuracy: 73.90465161165798%\n",
      "Training complete.\n",
      "Global Round: 3, Acc: 95.50827423167848\n",
      "Epoch 1/2, Loss: 271.4604758620262, Accuracy: 94.7703645216476%\n",
      "Epoch 2/2, Loss: 255.41885587573051, Accuracy: 95.19147256217923%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 354.65047711133957, Accuracy: 72.0903300521135%\n",
      "Epoch 2/2, Loss: 342.38234996795654, Accuracy: 74.71254859789892%\n",
      "Training complete.\n",
      "Global Round: 4, Acc: 95.65011820330969\n",
      "Epoch 1/2, Loss: 262.29630732536316, Accuracy: 95.05198052375312%\n",
      "Epoch 2/2, Loss: 246.4663050174713, Accuracy: 95.40992235820502%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 349.76168715953827, Accuracy: 73.0085201422781%\n",
      "Epoch 2/2, Loss: 337.3106528520584, Accuracy: 75.42393911820663%\n",
      "Training complete.\n",
      "Global Round: 5, Acc: 96.02836879432624\n",
      "Epoch 1/2, Loss: 253.2367775440216, Accuracy: 95.28095802079221%\n",
      "Epoch 2/2, Loss: 237.68748253583908, Accuracy: 95.72575338860376%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 344.7695987522602, Accuracy: 73.83571842170569%\n",
      "Epoch 2/2, Loss: 332.137853205204, Accuracy: 76.11327101772962%\n",
      "Training complete.\n",
      "Global Round: 6, Acc: 96.50118203309692\n",
      "Epoch 1/2, Loss: 244.26188668608665, Accuracy: 95.54678247137781%\n",
      "Epoch 2/2, Loss: 229.0475427210331, Accuracy: 95.94420318462956%\n",
      "Training complete.\n",
      "Epoch 1/2, Loss: 339.4902929663658, Accuracy: 74.64913006314279%\n",
      "Epoch 2/2, Loss: 326.6476290822029, Accuracy: 76.84120550362589%\n",
      "Training complete.\n",
      "Global Round: 7, Acc: 96.6903073286052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run = IID_simulation(2, 7, 0.00001, loader_01, loader_23, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
